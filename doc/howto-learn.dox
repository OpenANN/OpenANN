namespace OpenANN {

/**

\page HowtoLearn Applying Neural Networks

This is a short summary of best practices for applying multilayer neural
networks to arbitrary supervised learning problems. We will only consider
\b fully \b connected \b layers.

\section Architecture Network Architecture

The neural network should be as simple as possible to avoid overfitting.
Start with a linear network without hidden layers and only add hidden layers
or nodes if it improves the performance of the network. In principle, a
neural network with one hidden layer, a nonlinear activation function in the
hidden layer and a "sufficient" number of hidden units is able to approximate
arbitrary functions with arbitrary precision. In practice, adding more layers
can improve the performance of the neural network in terms of time. A few
number of hidden nodes is usually not sufficient to fit the training set good
enough. However, if the number of hidden nodes is to high, the generalization
is not good enough, i.e. the neural net overfits the training data. Tuning the
network architecture is not simple.

\section Functions Activation Functions and Error Functions

For regression problems, the error function that should be optimized is the
sum of squared errors (SSE) and in the output layer the activation function
should be linear (LINEAR). For multiclass classification problems, the error
function usually should be cross entropy (CE) and the activation function
linear (internally %OpenANN actually uses the softmax activation function in
combination with CE). Thus, the labels have to be represented through 1-of-c
encoding, that is to represent C classes C outputs are required. Each output
is binary and only one output should be 1, all other outputs have to be 0. The
index of the 1 indicates the actual class c. The predictions of the network
might not always be 0 or 1. Since the softmax activation function assures
that all outputs sum up to 1, we can even interpret the outputs as class
probabilities. To obtain the most likely predicted class, we compute the index
of the maximum value. However, for two classes, SSE and TANH activation
function sometimes work well enough, i.e. we only need one output and devide
its range into two regions of (usually) equal size and each region corresponds
to one of the two class.

In the hidden layers, nonlinear activation function are required. Available
options are:

- LOGISTIC
- TANH or TANH_SCALED
- RECTIFIER

We can distinguish saturating activation functions (sigmoid: LOGISTIC, TANH,
TANH_SCALED) and non-saturating activation functions (RECTIFIER). The
advantage of sigmoid activation function is that they generate more smooth
functions. Their disadvantage is that they do not work very well for deep
architectures because they make the error gradient of the first layers very
small.

\section Optimization Optimization Algorithm

We can choose between stochastic gradient descent (MBSGD), conjugate gradient
(CG) and Levenberg-Marquardt (LMA). LMA is usally the best algorithm because
it uses second-order information of the error function, i.e. it approximates
the second derivative. But it has some drawbacks:

- It works only for SSE.
- It has time complexity \f$ O(L^3) \f$, where L is the number of weights.
- It has space complexity \f$ O(LN) \f$, where N is the number of examples.

Thus, it is neither applicable for large nets, nor for large datasets. In this
case, we often use MBSGD because it has only \f$ O(L) \f$ time and space
complexity. It usually works very well for large redundant datasets for
classification. It might also be useful to take a look at conjugate gradient
for datasets that are not redundant, e.g. regression problems.

\section References

More tips can be found in the following documents. They are freely available.

[1] Sarle, W. S.:
Neural Network FAQ, postings to the Usenet newsgroup comp.ai.neural-nets,
1997,
ftp://ftp.sas.com/pub/neural/FAQ.html

[2] LeCun, Y.; Bottou, L.; Orr, G. B.; MÃ¼ller, K.-R.:
Efficient backprop,
Neural Networks: Tricks of the Trade. Springer, pp. 9-50.
*/

}
